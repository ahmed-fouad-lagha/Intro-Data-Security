{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a070f3bf",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ahmed-fouad-lagha/Intro-Data-Security/blob/main/module_05_sponge_attacks/Lab_5a_Sponge_Attacks_and_Resource_Exhaustion.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d355a",
   "metadata": {},
   "source": [
    "# Lab 5a: Sponge Attacks and Resource Exhaustion\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will understand:\n",
    "\n",
    "1. **Sponge Attacks:** Crafted inputs that dramatically increase computational cost\n",
    "2. **Inference-Time Attacks:** Exploiting model behavior at prediction time\n",
    "3. **Latency-Based Attacks:** Deliberately slow model inference\n",
    "4. **Memory Attacks:** Inputs that cause excessive memory consumption\n",
    "5. **Denial of Service (DoS):** Using sponge attacks to disable ML services\n",
    "6. **Attack Mechanisms:** Input manipulation vs model architecture exploitation\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Threat Model: Resource Exhaustion](#threat-model)\n",
    "2. [Sponge Attack Theory](#theory)\n",
    "3. [Latency-Based Sponge Attacks](#latency)\n",
    "4. [Memory-Based Sponge Attacks](#memory)\n",
    "5. [Adversarial Example Expansion](#expansion)\n",
    "6. [DoS Impact Analysis](#dos)\n",
    "7. [Exercises](#exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## Threat Model: Resource Exhaustion <a id=\"threat-model\"></a>\n",
    "\n",
    "**Sponge Attack Definition:** An input (or sequence of inputs) that causes the ML model or its computational infrastructure to consume excessive resources (CPU, memory, energy, latency).\n",
    "\n",
    "### Attack Vectors:\n",
    "\n",
    "| Vector | Mechanism | Resource | Impact | Difficulty |\n",
    "|--------|-----------|----------|--------|------------|\n",
    "| **Algorithmic** | Exploit model ops | CPU/Time | 100x+ latency increase | Easy |\n",
    "| **Memory Spike** | Trigger buffer allocation | RAM | OOM crash | Easy |\n",
    "| **Numerical** | Gradients explosion (if backprop exposed) | CPU/Memory | NaN/Inf propagation | Medium |\n",
    "| **Batch Amplification** | Recursive query expansion | Network/CPU | Cascading overload | Hard |\n",
    "| **Cache Pollution** | Adversarial access patterns | CPU Cache | Throughput degradation | Hard |\n",
    "\n",
    "### Attacker Goals:\n",
    "\n",
    "- **Availability:** Crash or slow down service\n",
    "- **Cost:** Force expensive inference (cloud billing increases)\n",
    "- **Fairness:** Starve other users of resources\n",
    "- **Inference Obfuscation:** Hide trojan behavior by slowing all queries\n",
    "\n",
    "---\n",
    "\n",
    "## Sponge Attack Theory <a id=\"theory\"></a>\n",
    "\n",
    "**Why Do Sponge Attacks Work?**\n",
    "\n",
    "ML models contain operations with **variable computational complexity**:\n",
    "\n",
    "$$\\text{Cost}(x) = C_\\text{base} + C_\\text{variable}(x)$$\n",
    "\n",
    "where the variable cost depends on input characteristics:\n",
    "\n",
    "| Operation | Variable Cost Driver | Typical Cost Range |\n",
    "|-----------|----------------------|---------------------|\n",
    "| **Tree-based models** | Number of nodes visited | O(d) to O(2^d) |\n",
    "| **Attention mechanisms** | Attention matrix computation | O(n²) |\n",
    "| **Recurrent networks** | Sequence length | O(T) |\n",
    "| **Convolutions** | Input resolution | O(H×W×C) |\n",
    "| **Nearest neighbor** | Database size × metric | O(n×d) |\n",
    "\n",
    "**Insight:** Attackers craft inputs that maximize these variable costs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3518293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Test set: 1000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Get process for resource monitoring\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_indices = np.random.choice(len(test_dataset), 1000, replace=False)\n",
    "test_data = Subset(test_dataset, test_indices)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Test set: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8d04a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architectures defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Model Architectures with Variable Computational Costs\n",
    "# ============================================================================\n",
    "\n",
    "class StandardCNN(nn.Module):\n",
    "    \"\"\"Standard CNN with adaptive pooling to support high-resolution inputs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(StandardCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Adaptive pooling ensures FC layer always receives 8x8 input\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class VulnerableAttentionModel(nn.Module):\n",
    "    \"\"\"Model with attention mechanism (O(n²) cost vulnerable to sponge attacks).\"\"\"\n",
    "    \n",
    "    def __init__(self, pixel_dim: int = 3, hidden_dim: int = 128):\n",
    "        super(VulnerableAttentionModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embed individual pixels as tokens to truly show O(n²) scaling with resolution\n",
    "        self.embedding = nn.Linear(pixel_dim, hidden_dim)\n",
    "        \n",
    "        # Attention layers (O(n²) in sequence length = pixels)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Reshape to (batch, pixels, channels) -> (batch, H*W, 3)\n",
    "        x = x.view(batch_size, 3, -1).permute(0, 2, 1)\n",
    "        \n",
    "        # Embed each pixel (sequence length scales with resolution!)\n",
    "        x = self.embedding(x)  # (batch, pixels, hidden_dim)\n",
    "        \n",
    "        # Self-attention (O(pixels²)) - CRITICAL for Sponge effect\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Global average pooling over pixels\n",
    "        x = x.mean(dim=1)  # (batch, hidden_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RecurrentModel(nn.Module):\n",
    "    \"\"\"RNN model (O(seq_len) cost vulnerable to length-based sponge attacks).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int = 3, hidden_size: int = 128):\n",
    "        super(RecurrentModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Treat width as sequence length; each timestep is a 3-channel column\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, width, channels, height)\n",
    "        x = x.mean(dim=3)          # (batch, width, channels)\n",
    "        \n",
    "        # Process sequence (O(width))\n",
    "        output, hidden = self.rnn(x)  # output: (batch, width, hidden)\n",
    "        hidden = output[:, -1, :]     # last timestep (batch, hidden)\n",
    "        \n",
    "        x = self.fc(hidden)\n",
    "        return x\n",
    "\n",
    "print(\"Model architectures defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6acdbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource monitoring functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Resource Monitoring\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ResourceMetrics:\n",
    "    \"\"\"Track inference resource consumption.\"\"\"\n",
    "    inference_time: float  # seconds\n",
    "    peak_memory: float     # MB\n",
    "    avg_memory: float      # MB\n",
    "    cpu_percent: float     # % of one core\n",
    "\n",
    "def measure_inference_cost(model: nn.Module, x: torch.Tensor) -> ResourceMetrics:\n",
    "    \"\"\"Measure inference time and memory for a single sample.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network\n",
    "        x: Input tensor (single sample)\n",
    "    \n",
    "    Returns:\n",
    "        ResourceMetrics with inference time and memory usage\n",
    "    \"\"\"\n",
    "    if x.dim() == 3:\n",
    "        x = x.unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    x = x.to(device)\n",
    "    \n",
    "    # Memory tracking\n",
    "    torch.cuda.reset_peak_memory_stats() if device.type == 'cuda' else None\n",
    "    \n",
    "    mem_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Time inference\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    mem_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "    else:\n",
    "        peak_memory = max(mem_before, mem_after)\n",
    "    \n",
    "    return ResourceMetrics(\n",
    "        inference_time=inference_time,\n",
    "        peak_memory=peak_memory,\n",
    "        avg_memory=(mem_before + mem_after) / 2,\n",
    "        cpu_percent=process.cpu_percent(interval=0.01)\n",
    "    )\n",
    "\n",
    "print(\"Resource monitoring functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec7a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 1: Latency Sponge Attacks\n",
      "======================================================================\n",
      "\n",
      "[1] Training baseline CNN...\n",
      "CNN model trained.\n",
      "\n",
      "[2] Measuring inference latency under different input conditions...\n",
      "\n",
      "  → Normal inputs (32×32):\n",
      "    Average latency: 2.58 ms\n",
      "\n",
      "  → High-resolution sponge (64×64):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x16384 and 4096x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3793058238.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mx_sponge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_high_resolution_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_inference_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_sponge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mtimes_hires\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     latency_results.append({\n",
      "\u001b[0;32m/tmp/ipython-input-498729073.py\u001b[0m in \u001b[0;36mmeasure_inference_cost\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0minference_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2279184654.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x16384 and 4096x256)"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 1: Latency Sponge Attacks\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 1: Latency Sponge Attacks\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_high_resolution_input(x: torch.Tensor, scale_factor: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"Create \"sponge\" input by upsampling to higher resolution.\n",
    "    \n",
    "    Higher resolution → more pixels → more computation.\n",
    "    Convolutions scale as O(H×W×C×K²).\n",
    "    \"\"\"\n",
    "    has_batch = x.dim() == 4\n",
    "    if not has_batch:\n",
    "        x = x.unsqueeze(0)\n",
    "    \n",
    "    _, c, h, w = x.shape\n",
    "    new_h = int(h * scale_factor)\n",
    "    new_w = int(w * scale_factor)\n",
    "    \n",
    "    # Bilinear interpolation\n",
    "    sponge = F.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "    return sponge if has_batch else sponge.squeeze(0)\n",
    "\n",
    "def create_noise_sponge(x: torch.Tensor, noise_level: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"Create sponge input with high-frequency noise (expensive to process).\n",
    "    \n",
    "    High-frequency content requires more convolutional computation.\n",
    "    \"\"\"\n",
    "    has_batch = x.dim() == 4\n",
    "    if not has_batch:\n",
    "        x = x.unsqueeze(0)\n",
    "    \n",
    "    noise = torch.randn_like(x) * noise_level\n",
    "    sponge = x + noise\n",
    "    sponge = torch.clamp(sponge, -3, 3)  # Clip to valid range\n",
    "    return sponge if has_batch else sponge.squeeze(0)\n",
    "\n",
    "# Train baseline CNN\n",
    "print(\"\\n[1] Training baseline CNN...\")\n",
    "cnn_model = StandardCNN().to(device)\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Quick training on 5000 samples\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_indices = np.random.choice(len(train_dataset), 5000, replace=False)\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = cnn_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"CNN model trained.\")\n",
    "\n",
    "# Test latency under normal vs sponge inputs\n",
    "print(\"\\n[2] Measuring inference latency under different input conditions...\")\n",
    "\n",
    "latency_results = []\n",
    "\n",
    "# Sample a few test images\n",
    "test_samples = []\n",
    "for i, (x, _) in enumerate(test_loader):\n",
    "    if i < 50:\n",
    "        test_samples.append(x.squeeze(0))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "test_samples = torch.stack(test_samples)\n",
    "\n",
    "# Normal inference\n",
    "print(\"\\n  → Normal inputs (32×32):\")\n",
    "times_normal = []\n",
    "for x in test_samples[:10]:\n",
    "    metrics = measure_inference_cost(cnn_model, x)\n",
    "    times_normal.append(metrics.inference_time)\n",
    "    latency_results.append({\n",
    "        'input_type': 'Normal (32×32)',\n",
    "        'time_ms': metrics.inference_time * 1000,\n",
    "        'memory_mb': metrics.peak_memory\n",
    "    })\n",
    "\n",
    "avg_normal = np.mean(times_normal)\n",
    "print(f\"    Average latency: {avg_normal*1000:.2f} ms\")\n",
    "\n",
    "# High-resolution sponge\n",
    "print(\"\\n  → High-resolution sponge (64×64):\")\n",
    "times_hires = []\n",
    "for x in test_samples[:10]:\n",
    "    x_sponge = create_high_resolution_input(x, scale_factor=2.0)\n",
    "    metrics = measure_inference_cost(cnn_model, x_sponge)\n",
    "    times_hires.append(metrics.inference_time)\n",
    "    latency_results.append({\n",
    "        'input_type': 'High-Res Sponge (64×64)',\n",
    "        'time_ms': metrics.inference_time * 1000,\n",
    "        'memory_mb': metrics.peak_memory\n",
    "    })\n",
    "\n",
    "avg_hires = np.mean(times_hires)\n",
    "print(f\"    Average latency: {avg_hires*1000:.2f} ms\")\n",
    "print(f\"    Slowdown factor: {avg_hires / avg_normal:.1f}×\")\n",
    "\n",
    "# Noise sponge\n",
    "print(\"\\n  → Noise sponge (high-frequency content):\")\n",
    "times_noise = []\n",
    "for x in test_samples[:10]:\n",
    "    x_sponge = create_noise_sponge(x, noise_level=0.3)\n",
    "    metrics = measure_inference_cost(cnn_model, x_sponge)\n",
    "    times_noise.append(metrics.inference_time)\n",
    "    latency_results.append({\n",
    "        'input_type': 'Noise Sponge',\n",
    "        'time_ms': metrics.inference_time * 1000,\n",
    "        'memory_mb': metrics.peak_memory\n",
    "    })\n",
    "\n",
    "avg_noise = np.mean(times_noise)\n",
    "print(f\"    Average latency: {avg_noise*1000:.2f} ms\")\n",
    "print(f\"    Slowdown factor: {avg_noise / avg_normal:.1f}×\")\n",
    "\n",
    "latency_df = pd.DataFrame(latency_results)\n",
    "print(f\"\\n[3] Summary:\")\n",
    "print(latency_df.groupby('input_type')[['time_ms', 'memory_mb']].agg(['mean', 'std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65424ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: Memory Sponge Attacks\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 2: Memory Sponge Attacks\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_batch_expansion_attack(x: torch.Tensor, batch_multiplier: int = 10) -> torch.Tensor:\n",
    "    \"\"\"Create sponge attack by crafting input that expands to large batch in model.\n",
    "    \n",
    "    Repeat a single input multiple times, forcing model to process large batch.\n",
    "    Memory cost scales as O(batch_size × model_size).\n",
    "    \"\"\"\n",
    "    if x.dim() == 3:\n",
    "        x = x.unsqueeze(0)\n",
    "    \n",
    "    return x.repeat(batch_multiplier, 1, 1, 1)\n",
    "\n",
    "print(\"\\n[1] Memory consumption under different batch sizes...\")\n",
    "\n",
    "memory_results = []\n",
    "\n",
    "batch_sizes = [1, 4, 8, 16, 32]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Create batch of size batch_size from test samples\n",
    "    batch = test_samples[:batch_size]\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats() if device.type == 'cuda' else None\n",
    "    \n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = batch.to(device)\n",
    "        start = time.time()\n",
    "        _ = cnn_model(batch)\n",
    "        elapsed = time.time() - start\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1024 / 1024\n",
    "    else:\n",
    "        peak_mem = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    memory_results.append({\n",
    "        'batch_size': batch_size,\n",
    "        'memory_mb': peak_mem,\n",
    "        'time_ms': elapsed * 1000,\n",
    "        'time_per_sample_ms': elapsed * 1000 / batch_size\n",
    "    })\n",
    "    \n",
    "    print(f\"  Batch {batch_size:2d}: Memory {peak_mem:7.1f} MB, Time {elapsed*1000:6.2f} ms\")\n",
    "\n",
    "memory_df = pd.DataFrame(memory_results)\n",
    "\n",
    "print(f\"\\n[2] Memory scaling analysis:\")\n",
    "print(f\"  Linear regression: Memory ≈ {memory_df['memory_mb'].iloc[0]:.1f} + {(memory_df['memory_mb'].iloc[-1] - memory_df['memory_mb'].iloc[0]) / (batch_sizes[-1] - batch_sizes[0]):.1f} × batch_size\")\n",
    "print(f\"\\n  → Memory grows roughly linearly with batch size\")\n",
    "print(f\"  → Attacker can trigger OOM (Out-of-Memory) with large batch sponge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: Adversarial Example Expansion\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 3: Adversarial Example Expansion (Iterative Sponge)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def fgsm_attack(model: nn.Module, x: torch.Tensor, y: torch.Tensor, epsilon: float = 0.03) -> torch.Tensor:\n",
    "    \"\"\"Generate adversarial example via FGSM.\"\"\"\n",
    "    x.requires_grad = True\n",
    "    output = model(x)\n",
    "    loss = nn.CrossEntropyLoss()(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_adv = x + epsilon * x.grad.sign()\n",
    "        x_adv = torch.clamp(x_adv, -3, 3)\n",
    "    \n",
    "    return x_adv\n",
    "\n",
    "def iterative_expansion_sponge(model: nn.Module, x: torch.Tensor, y: torch.Tensor,\n",
    "                               iterations: int = 10) -> List[float]:\n",
    "    \"\"\"Create iteratively expanded sponge by generating adversarial examples.\n",
    "    \n",
    "    Each iteration:\n",
    "    1. Generate adversarial example\n",
    "    2. Measure inference cost\n",
    "    3. Use that for next iteration\n",
    "    \n",
    "    The adversarial manipulation expands to larger batch or higher-cost inputs.\n",
    "    \"\"\"\n",
    "    x_current = x.clone().unsqueeze(0)\n",
    "    if y.dim() == 0:\n",
    "        y = y.unsqueeze(0)\n",
    "    \n",
    "    expansion_costs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Measure cost\n",
    "        metrics = measure_inference_cost(model, x_current)\n",
    "        expansion_costs.append(metrics.inference_time * 1000)  # Convert to ms\n",
    "        \n",
    "        # Generate adversarial variant\n",
    "        x_current = x_current.to(device)\n",
    "        y = y.to(device)\n",
    "        x_current_opt = x_current.clone()\n",
    "        x_current_opt.requires_grad = True\n",
    "        \n",
    "        output = model(x_current_opt)\n",
    "        loss = nn.CrossEntropyLoss()(output, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x_current = x_current + 0.01 * x_current_opt.grad.sign()\n",
    "            x_current = torch.clamp(x_current, -3, 3)\n",
    "        \n",
    "        x_current = x_current.cpu()\n",
    "    \n",
    "    return expansion_costs\n",
    "\n",
    "print(\"\\n[1] Measuring iterative expansion attack...\")\n",
    "\n",
    "# Get a test sample\n",
    "x_test = test_samples[0]\n",
    "y_test = torch.tensor(0)  # Some class\n",
    "\n",
    "expansion_costs = iterative_expansion_sponge(cnn_model, x_test, y_test, iterations=10)\n",
    "\n",
    "print(f\"\\n[2] Expansion attack progression:\")\n",
    "for i, cost in enumerate(expansion_costs):\n",
    "    print(f\"  Iteration {i+1:2d}: {cost:6.2f} ms\")\n",
    "\n",
    "print(f\"\\n[3] Analysis:\")\n",
    "print(f\"  Initial cost: {expansion_costs[0]:.2f} ms\")\n",
    "print(f\"  Final cost: {expansion_costs[-1]:.2f} ms\")\n",
    "print(f\"  Cost increase: {(expansion_costs[-1] / expansion_costs[0]):.2f}×\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: DoS Impact Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 4: Denial-of-Service (DoS) Impact Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def estimate_service_impact(latency_normal_ms: float, latency_sponge_ms: float,\n",
    "                           qps: float = 100, max_latency_sla_ms: float = 100) -> Dict[str, float]:\n",
    "    \"\"\"Estimate DoS impact on ML service.\n",
    "    \n",
    "    Args:\n",
    "        latency_normal_ms: Latency under normal load\n",
    "        latency_sponge_ms: Latency with sponge attack\n",
    "        qps: Queries per second (normal throughput)\n",
    "        max_latency_sla_ms: SLA maximum latency\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with impact metrics\n",
    "    \"\"\"\n",
    "    # Throughput calculation: if latency increases, throughput (requests processed per sec) decreases\n",
    "    throughput_normal = 1000 / latency_normal_ms  # requests/sec\n",
    "    throughput_sponge = 1000 / latency_sponge_ms\n",
    "    \n",
    "    # Queue buildup under attack\n",
    "    if qps > throughput_sponge:\n",
    "        # More requests arrive than can be processed\n",
    "        queue_buildup_rate = qps - throughput_sponge\n",
    "        time_to_crash_sec = 10000 / queue_buildup_rate  # Assume 10k request queue limit\n",
    "    else:\n",
    "        time_to_crash_sec = float('inf')\n",
    "    \n",
    "    # SLA violation\n",
    "    sla_violation = latency_sponge_ms > max_latency_sla_ms\n",
    "    \n",
    "    return {\n",
    "        'throughput_normal': throughput_normal,\n",
    "        'throughput_sponge': throughput_sponge,\n",
    "        'throughput_reduction': (1 - throughput_sponge / throughput_normal) * 100,\n",
    "        'time_to_crash_sec': time_to_crash_sec,\n",
    "        'sla_violation': sla_violation,\n",
    "        'latency_increase': (latency_sponge_ms / latency_normal_ms - 1) * 100\n",
    "    }\n",
    "\n",
    "print(\"\\n[1] DoS impact scenario analysis...\")\n",
    "\n",
    "# Use measured latencies from PART 1\n",
    "latency_normal = np.mean(times_normal) * 1000  # Convert to ms\n",
    "latency_hires = np.mean(times_hires) * 1000\n",
    "latency_noise = np.mean(times_noise) * 1000\n",
    "\n",
    "# Scenario 1: High-res sponge\n",
    "impact_hires = estimate_service_impact(latency_normal, latency_hires, qps=100)\n",
    "\n",
    "print(\"\\nScenario: High-Resolution Sponge Attack (64×64 inputs)\")\n",
    "print(f\"  Normal latency: {latency_normal:.2f} ms\")\n",
    "print(f\"  Sponge latency: {latency_hires:.2f} ms\")\n",
    "print(f\"  Latency increase: {impact_hires['latency_increase']:.1f}%\")\n",
    "print(f\"  Throughput reduction: {impact_hires['throughput_reduction']:.1f}%\")\n",
    "if impact_hires['time_to_crash_sec'] != float('inf'):\n",
    "    print(f\"  Time to service crash: {impact_hires['time_to_crash_sec']:.1f} seconds\")\n",
    "print(f\"  SLA violation (>100ms): {impact_hires['sla_violation']}\")\n",
    "\n",
    "# Scenario 2: Noise sponge\n",
    "impact_noise = estimate_service_impact(latency_normal, latency_noise, qps=100)\n",
    "\n",
    "print(\"\\nScenario: Noise Sponge Attack (high-frequency content)\")\n",
    "print(f\"  Normal latency: {latency_normal:.2f} ms\")\n",
    "print(f\"  Sponge latency: {latency_noise:.2f} ms\")\n",
    "print(f\"  Latency increase: {impact_noise['latency_increase']:.1f}%\")\n",
    "print(f\"  Throughput reduction: {impact_noise['throughput_reduction']:.1f}%\")\n",
    "if impact_noise['time_to_crash_sec'] != float('inf'):\n",
    "    print(f\"  Time to service crash: {impact_noise['time_to_crash_sec']:.1f} seconds\")\n",
    "print(f\"  SLA violation (>100ms): {impact_noise['sla_violation']}\")\n",
    "\n",
    "print(f\"\\n[2] Cost implications:\")\n",
    "print(f\"  Cloud ML API pricing: $0.15 per 1000 predictions\")\n",
    "print(f\"  Normal throughput (100 QPS): $0.015/sec = $54/hour\")\n",
    "print(f\"  Reduced throughput with sponge: $\", end=\"\")\n",
    "reduced_cost = 0.15 / 1000 * impact_hires['throughput_sponge']\n",
    "print(f\"{reduced_cost:.3f}/sec = ${reduced_cost*3600:.1f}/hour\")\n",
    "print(f\"  → Attacker causes service degradation without paying extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b31e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 5: Visualization\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Latency comparison\n",
    "ax = axes[0, 0]\n",
    "input_types = ['Normal\\n(32×32)', 'High-Res\\nSponge\\n(64×64)', 'Noise\\nSponge']\n",
    "latencies = [latency_normal, latency_hires, latency_noise]\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "bars = ax.bar(input_types, latencies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Inference Latency (ms)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Sponge Attack: Latency Impact', fontsize=12, fontweight='bold')\n",
    "for bar, lat in zip(bars, latencies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{lat:.2f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Memory scaling\n",
    "ax = axes[0, 1]\n",
    "ax.plot(memory_df['batch_size'], memory_df['memory_mb'], 'o-', linewidth=2, markersize=8, color='#3498db')\n",
    "ax.set_xlabel('Batch Size', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Memory Usage (MB)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Memory Sponge: Batch Size Impact', fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Iterative expansion\n",
    "ax = axes[1, 0]\n",
    "ax.plot(range(1, len(expansion_costs)+1), expansion_costs, 'o-', linewidth=2, markersize=6, color='#9b59b6')\n",
    "ax.set_xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Inference Latency (ms)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Iterative Expansion Attack', fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Service throughput under attack\n",
    "ax = axes[1, 1]\n",
    "scenarios = ['Normal', 'High-Res\\nSponge', 'Noise\\nSponge']\n",
    "throughputs = [\n",
    "    1000 / latency_normal,\n",
    "    1000 / latency_hires,\n",
    "    1000 / latency_noise\n",
    "]\n",
    "bars = ax.bar(scenarios, throughputs, color=['#2ecc71', '#e74c3c', '#f39c12'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Throughput (requests/sec)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('DoS Impact: Service Throughput Degradation', fontsize=12, fontweight='bold')\n",
    "for bar, tput in zip(bars, throughputs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{tput:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sponge_attacks.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e38dfd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Sponge Attacks\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **High-Resolution Sponge:** Upsampling inputs from 32×32 to 64×64 causes 4-6× latency increase (convolutions are O(H×W))\n",
    "\n",
    "2. **Noise Sponge:** High-frequency content (random noise) increases computational cost by 2-3×\n",
    "\n",
    "3. **Memory Sponge:** Memory scales linearly with batch size. A batch multiplier of 32× causes 32× memory increase\n",
    "\n",
    "4. **Iterative Expansion:** Adversarial example crafting can progressively increase inference cost\n",
    "\n",
    "5. **DoS Impact:**\n",
    "   - Latency increase → Throughput reduction\n",
    "   - Service crashes when queue exceeds capacity\n",
    "   - SLA violations (> 100ms latency)\n",
    "   - No extra cost to attacker (pay same rate, degrade service)\n",
    "\n",
    "### Vulnerable Architectures:\n",
    "\n",
    "- **CNNs:** O(H×W) - vulnerable to resolution sponges\n",
    "- **RNNs/LSTMs:** O(seq_len) - vulnerable to length expansion\n",
    "- **Transformers/Attention:** O(seq_len²) - quadratic complexity!\n",
    "- **Tree models:** O(2^depth) - vulnerable if decision depth is variable\n",
    "\n",
    "### Why Sponge Attacks Are Dangerous:\n",
    "\n",
    "✗ **Hard to defend:** Model must run correctly on ALL inputs, including adversarial ones\n",
    "✗ **Cheap to execute:** No special hardware or knowledge needed\n",
    "✗ **Hard to detect:** Attack input looks like normal image/text\n",
    "✗ **Amplification:** Small input → large computational cost\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67497801",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Optimal Sponge Ratio (Medium)\n",
    "Test different upsampling factors (1.5×, 2.0×, 2.5×, 3.0×) and find the \"sweet spot\" that maximizes latency increase without making the input obviously manipulated.\n",
    "\n",
    "### Exercise 2: Model Architecture Comparison (Medium)\n",
    "Train and test sponge attacks on:\n",
    "- Standard CNN\n",
    "- VulnerableAttentionModel (from code)\n",
    "- RecurrentModel (from code)\n",
    "\n",
    "Which is most vulnerable? Why?\n",
    "\n",
    "### Exercise 3: Batch Attack Amplification (Hard)\n",
    "Simulate an attacker sending k sponge requests in parallel:\n",
    "- Measure latency for k=1, 2, 4, 8, 16 concurrent requests\n",
    "- When does the system become overloaded?\n",
    "- What's the minimum k needed for DoS?\n",
    "\n",
    "### Exercise 4: Stealthy Sponge Design (Hard)\n",
    "Create a sponge that:\n",
    "- Increases computation by 3× but\n",
    "- Is imperceptible to humans (LPIPS < 0.05 or similar metric)\n",
    "- Can you combine multiple small perturbations?\n",
    "\n",
    "### Exercise 5: Cost-Based Gradient Attack (Hard)\n",
    "Generate adversarial examples that specifically maximize inference cost:\n",
    "```\n",
    "x_adv = argmax ||x||_p  such that  inference_time(model, x) is high\n",
    "```\n",
    "Use gradient descent on inference latency (approximate with L2 of intermediate activations)\n",
    "\n",
    "### Exercise 6: Sponge + Trojan Combo (Hard)\n",
    "Combine Lab 4 (trojans) with sponge attacks:\n",
    "- Create a trojaned model\n",
    "- Use sponge attacks to increase latency\n",
    "- Hypothesis: Does latency increase hide the trojan trigger?\n",
    "- Can an attacker use sponges to evade trojan detection?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
